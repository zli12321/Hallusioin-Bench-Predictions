# -*- coding: utf-8 -*-
"""Another copy of Inference with ViLT (visual question answering).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b3VnFZ7hHcE0E-jVERIMebofD8DTEk2-

# Performing visual question answering (VQA) with ViLT

In this notebook, we are going to illustate visual question answering with the Vision-and-Language Transformer (ViLT). This model is very minimal: it only adds text embedding layers to an existing ViT model. It does not require any sophisticated CNN-based pipelines to feed the image to the model (unlike models like [PixelBERT](https://arxiv.org/abs/2004.00849) and [LXMERT](https://arxiv.org/abs/1908.07490)). This makes the model also much faster than previous works.

![ViLT architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vilt_architecture.jpg)

Figure taken from the original [paper](https://arxiv.org/abs/2102.03334).

HuggingFace docs: https://huggingface.co/docs/transformers/master/en/model_doc/vilt

## Set-up environment

First, we install HuggingFace Transformers.
"""

!pip install -q git+https://github.com/huggingface/transformers.git

!pip install transformers

"""## Prepare image + question

Here we take our familiar cats image (of the COCO dataset) and create a corresponding question.
"""

import requests
from PIL import Image

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
text = "Describe what are in the images?"
image

!pip install transformers

from transformers import ViltProcessor

processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")

"""Let's prepare the image+text pair for the model. Here, we leverage `ViltProcessor`, which will use (behind the scenes):
* `BertTokenizerFast` to tokenize the text (and create input_ids, attention_mask, token_type_ids)
* `ViltFeatureExtractor` to resize + normalize the image (and create pixel_values and pixel_mask).

Note that the `pixel_mask` is only relevant in case of batches, as it can be used to indicate which pixels are real/which are padding. Here we're only preparing a single example for the model, hence all values of pixel_mask will be 1.
"""

encoding = processor(image, text, return_tensors="pt")
for k,v in encoding.items():
  print(k, v.shape)

"""## Define model

Here we load the ViLT model, fine-tuned on VQAv2, from the [hub](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa).
"""

from transformers import ViltForQuestionAnswering

model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")

"""## Forward pass

We can now forward both the `input_ids` and `pixel_values` through the model. The model outputs logits of shape (batch_size, num_labels), which in this case will be (1, 3129) - as the VQAv2 dataset has 3129 possible answers.
"""

import torch

# forward pass
outputs = model(**encoding)
logits = outputs.logits
idx = torch.sigmoid(logits).argmax(-1).item()
print("Predicted answer:", model.config.id2label[idx])

"""## Inference on hallusion Bench"""

from google.colab import drive
drive.mount('/content/drive')

import json
with open('/content/drive/MyDrive/HallusionBench/HallusionBench.json', 'r') as f:
    data = json.load(f)

for i, element in enumerate(data):
    try:
        img_path = '/content/drive/MyDrive/HallusionBench/' + data[i]['filename']
        raw_image = Image.open(img_path).convert('RGB')
        text = data[i]['question']

        encoding = processor(raw_image, text, return_tensors="pt")

        outputs = model(**encoding)
        logits = outputs.logits
        idx = torch.sigmoid(logits).argmax(-1).item()
        print("Predicted answer:", model.config.id2label[idx])

        result = model.config.id2label[idx]

        element['model_prediction'] = result
    except:
        element['model_prediction'] = 'null'

output_json = json.dumps(data, indent=4)

# Save the output JSON to a file
with open('./ViLT.json', 'w') as f:
    f.write(output_json)