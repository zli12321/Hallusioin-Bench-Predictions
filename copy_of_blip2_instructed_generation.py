# -*- coding: utf-8 -*-
"""Copy of blip2_instructed_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10kNNwEx07Td61-sCLuX0h9HdqiFepV4G

#### Large RAM is required to load the larger models. Running on GPU can optimize inference speed.
"""

import sys
if 'google.colab' in sys.modules:
    print('Running in Colab.')
    !pip3 install salesforce-lavis

import torch
from PIL import Image
import requests
from lavis.models import load_model_and_preprocess

"""#### Load an example image"""

img_url = 'https://storage.googleapis.com/sfr-vision-language-research/LAVIS/assets/merlion.png'
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')
display(raw_image.resize((596, 437)))

# setup device to use
device = torch.device("cuda") if torch.cuda.is_available() else "cpu"

"""#### Load pretrained/finetuned BLIP2 captioning model"""

# we associate a model with its preprocessors to make it easier for inference.
model, vis_processors, _ = load_model_and_preprocess(
    name="blip2_t5", model_type="pretrain_flant5xxl", is_eval=True, device=device
)

# Other available models:
#
# model, vis_processors, _ = load_model_and_preprocess(
#     name="blip2_opt", model_type="pretrain_opt2.7b", is_eval=True, device=device
# )
# model, vis_processors, _ = load_model_and_preprocess(
#     name="blip2_opt", model_type="pretrain_opt6.7b", is_eval=True, device=device
# )
# model, vis_processors, _ = load_model_and_preprocess(
#     name="blip2_opt", model_type="caption_coco_opt2.7b", is_eval=True, device=device
# )
# model, vis_processors, _ = load_model_and_preprocess(
#     name="blip2_opt", model_type="caption_coco_opt6.7b", is_eval=True, device=device
# )
#
# model, vis_processors, _ = load_model_and_preprocess(
#     name="blip2_t5", model_type="pretrain_flant5xl", is_eval=True, device=device
# )
#
# model, vis_processors, _ = load_model_and_preprocess(
#     name="blip2_t5", model_type="caption_coco_flant5xl", is_eval=True, device=device
# )

vis_processors.keys()

"""#### prepare the image as model input using the associated processors"""

image = vis_processors["eval"](raw_image).unsqueeze(0).to(device)

"""#### generate caption using beam search"""

model.generate({"image": image})

"""#### generate multiple captions using nucleus sampling"""

# due to the non-determinstic nature of necleus sampling, you may get different captions.
model.generate({"image": image}, use_nucleus_sampling=True, num_captions=3)

from google.colab import drive
drive.mount('/content/drive')

img_path = '/content/drive/MyDrive/HallusionBench/hallusion_bench/VD/figure/0_0.png'
raw_image = Image.open(img_path).convert('RGB')
display(raw_image.resize((596, 437)))

image = vis_processors["eval"](raw_image).unsqueeze(0).to(device)
model.generate({
    "image": image,
    "prompt": "Question: which city is this? "})

import json
with open('/content/drive/MyDrive/HallusionBench/HallusionBench.json', 'r') as f:
    data = json.load(f)

for i, element in enumerate(data):
    try:
        img_path = '/content/drive/MyDrive/HallusionBench/' + data[i]['filename']
        raw_image = Image.open(img_path).convert('RGB')
        image = vis_processors["eval"](raw_image).unsqueeze(0).to(device)
        result = model.generate({
          "image": image,
          "prompt": data[i]['question']})[0]

        element['model_prediction'] = result
    except:
        element['model_prediction'] = 'null'

output_json = json.dumps(data, indent=4)

# Save the output JSON to a file
with open('./BLIP-VL.json', 'w') as f:
    f.write(output_json)

"""#### instructed zero-shot vision-to-language generation"""

model.generate({"image": image, "prompt": "Question: which city is this? Answer:"})

model.generate({
    "image": image,
    "prompt": "Question: which city is this? Answer: singapore. Question: why?"})

context = [
    ("which city is this?", "singapore"),
    ("why?", "it has a statue of a merlion"),
]
question = "where is the name merlion coming from?"
template = "Question: {} Answer: {}."

prompt = " ".join([template.format(context[i][0], context[i][1]) for i in range(len(context))]) + " Question: " + question + " Answer:"

print(prompt)

model.generate(
    {
    "image": image,
    "prompt": prompt
    },
    use_nucleus_sampling=False,
)